#!/bin/bash
set -euo pipefail

log() {
  echo "$(date '+%Y-%m-%d %H:%M:%S') $*"
}

if [[ $# -ne 3 ]]; then
  echo "Usage: $0 <openrc_file> <tag> <ssh_key_name>"
  exit 1
fi

OPENRC="$1"
TAG="$2"
KEY="$3"
KEY_PRIV="$HOME/.ssh/$KEY"

_bastion="${TAG}_bastion"
_proxy="${TAG}_proxy"
_node="${TAG}_node"
_key="$KEY"
_sg_nodes="sg_nodes_${TAG}"
NET="${TAG}_network"

if [ ! -f "$OPENRC" ]; then
  log "ERROR: OpenRC file $OPENRC not found."
  exit 1
fi

if [ ! -f "$KEY_PRIV" ]; then
  log "ERROR: Private key $KEY_PRIV not found. Make sure it's in ~/.ssh/ and named '$KEY'."
  exit 1
fi

source "$OPENRC"

log "Starting operations for $TAG using $OPENRC for credentials."

BASTION_FIP=""
PROXY_FIP=""
BASTION_INTERNAL_IP=""
PROXY_INTERNAL_IP=""

wait_for_ssh() {
  local ip="$1"
  local timeout=180
  local interval=5
  local waited=0
  echo -n "  Waiting for SSH on $ip "
  while ! ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 -i "$KEY_PRIV" "ubuntu@$ip" 'echo ok' >/dev/null 2>&1; do
    if [ "$waited" -ge "$timeout" ]; then
      echo ""
      log "ERROR: Timeout waiting for SSH on $ip"
      return 1
    fi
    sleep "$interval"
    waited=$((waited + interval))
    echo -n "."
  done
  echo ""
  log "  SSH is ready on $ip."
  return 0
}

wait_for_internal_ssh_from_bastion() {
  local node_ip="$1"
  local remote_cmd_timeout=180
  local connect_timeout_per_try=10
  local retries=$((remote_cmd_timeout / (connect_timeout_per_try + 2) ))

  echo -n "  Initiating SSH wait from Bastion to $node_ip "

  local remote_script
  remote_script=$(cat <<'REMOTE_EOF'
    node_ip_to_check="$1"
    key_to_use="$2" 
    max_attempts="$3"
    attempt_connect_timeout="$4"
    attempts=0
    mkdir -p "$HOME/.ssh" && chmod 700 "$HOME/.ssh"
    key_file_path="$HOME/.ssh/$key_to_use"

    if [ ! -f "$key_file_path" ]; then
        echo "ERROR: Key file $key_file_path not found on bastion." >&2
        exit 2 
    fi

    while true; do
      if ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout="$attempt_connect_timeout" -i "$key_file_path" "ubuntu@$node_ip_to_check" 'echo ok' >/dev/null 2>&1; then
        exit 0 
      fi
      attempts=$((attempts + 1))
      if [ "$attempts" -ge "$max_attempts" ]; then
        exit 1 
      fi
      sleep 2 
    done
REMOTE_EOF
)

  if timeout "$remote_cmd_timeout" ssh -o StrictHostKeyChecking=no -i "$KEY_PRIV" "ubuntu@$BASTION_FIP" \
    /bin/bash -s -- "$node_ip" "$_key" "$retries" "$connect_timeout_per_try" <<< "$remote_script"; then
    echo "" 
    log "  SSH from Bastion to $node_ip is ready."
    return 0
  else
    local exit_status=$?
    echo ""
    log "ERROR: SSH wait from Bastion to $node_ip failed. Exit status $exit_status."
    if [[ "$exit_status" -eq 124 ]]; then log "  Reason: Overall timeout for SSH command to bastion."; fi
    if [[ "$exit_status" -eq 2 ]]; then log "  Reason: Key file likely missing on bastion."; fi 
    return 1
  fi
}


get_fips_and_internal_ips() {
  BASTION_FIP=""
  PROXY_FIP=""
  BASTION_INTERNAL_IP=""
  PROXY_INTERNAL_IP=""
  BASTION_FIP=$(openstack server show "$_bastion" -f value -c addresses | grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}' | grep -vE '^(10\.|172\.(1[6-9]|2[0-9]|3[0-1])\.|192\.168\.)' | head -1 || true)
  if [[ -z "$BASTION_FIP" ]]; then
    log "ERROR: Could not find floating IP for Bastion server $_bastion. Exiting."
    exit 1
  fi
  PROXY_FIP=$(openstack server show "$_proxy" -f value -c addresses | grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}' | grep -vE '^(10\.|172\.(1[6-9]|2[0-9]|3[0-1])\.|192\.168\.)' | head -1 || true)
  if [[ -z "$PROXY_FIP" ]]; then
    log "ERROR: Could not find floating IP for Proxy server $_proxy. Exiting."
    exit 1
  fi
  BASTION_INTERNAL_IP=$(openstack server show "$_bastion" -f value -c addresses | grep "$NET" | grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}' | head -1 || true)
  PROXY_INTERNAL_IP=$(openstack server show "$_proxy" -f value -c addresses | grep "$NET" | grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}' | head -1 || true)
  if [[ -z "$BASTION_INTERNAL_IP" ]]; then log "ERROR: Could not get internal IP for Bastion ($_bastion) on network $NET. Exiting."; exit 1; fi
  if [[ -z "$PROXY_INTERNAL_IP" ]]; then log "ERROR: Could not get internal IP for Proxy ($_proxy) on network $NET. Exiting."; exit 1; fi
  
  if [[ -z "${_FIPS_LOGGED_ONCE:-}" ]]; then
    log "Found essential IPs: Bastion FIP:$BASTION_FIP, Proxy FIP:$PROXY_FIP, Bastion Internal IP:$BASTION_INTERNAL_IP, Proxy Internal IP:$PROXY_INTERNAL_IP"
    _FIPS_LOGGED_ONCE=true
  fi
}

update_and_run_ansible() {
  log "Updating Ansible configuration on Bastion and running playbook..."
  local INVENTORY_FILE="hosts.ini"
  echo "[nodes]" > "$INVENTORY_FILE"
  mapfile -t active_node_instances < <(openstack server list --long --status ACTIVE --name "$_node" -f value -c Name -c Networks)
  for line in "${active_node_instances[@]}"; do
    name="${line%% *}"
    ip=$(echo "$line" | grep "$NET" | grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}' | head -1 || true)
    if [[ -n "$name" && -n "$ip" ]]; then
      echo "$name ansible_host=$ip ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/$_key ansible_python_interpreter=/usr/bin/python3" >> "$INVENTORY_FILE"
    fi
  done
  echo -e "\n[proxy]" >> "$INVENTORY_FILE"
  echo "$_proxy ansible_host=$PROXY_INTERNAL_IP ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/$_key ansible_python_interpreter=/usr/bin/python3" >> "$INVENTORY_FILE"
  echo -e "\n[bastion]" >> "$INVENTORY_FILE"
  echo "$_bastion ansible_host=$BASTION_INTERNAL_IP ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/$_key ansible_python_interpreter=/usr/bin/python3" >> "$INVENTORY_FILE"
  echo -e "\n[all:children]" >> "$INVENTORY_FILE"
  echo "nodes" >> "$INVENTORY_FILE"
  echo "proxy" >> "$INVENTORY_FILE"
  echo "bastion" >> "$INVENTORY_FILE"

  log "Preparing Bastion for Ansible operations..."
  scp -o StrictHostKeyChecking=no -i "$KEY_PRIV" "$INVENTORY_FILE" "ubuntu@$BASTION_FIP":~ >/dev/null || { log "ERROR: Failed to copy hosts.ini."; exit 1; }
  ssh -o StrictHostKeyChecking=no -i "$KEY_PRIV" "ubuntu@$BASTION_FIP" "mkdir -p ~/.ssh/ && chmod 700 ~/.ssh/" >/dev/null || { log "ERROR: Failed to create .ssh dir on bastion."; exit 1; }
  scp -o StrictHostKeyChecking=no -i "$KEY_PRIV" "$KEY_PRIV" "ubuntu@$BASTION_FIP:~/.ssh/$_key" >/dev/null || { log "ERROR: Failed to copy private key."; exit 1; }
  ssh -o StrictHostKeyChecking=no -i "$KEY_PRIV" "ubuntu@$BASTION_FIP" "chmod 600 ~/.ssh/$_key" >/dev/null || { log "ERROR: Failed to set key permissions."; exit 1; }
  
  ssh -o StrictHostKeyChecking=no -i "$KEY_PRIV" "ubuntu@$BASTION_FIP" "
    mkdir -p ~/.ssh
    touch ~/.ssh/config
    if ! grep -q 'StrictHostKeyChecking no' ~/.ssh/config; then
      echo -e 'Host *\\n  StrictHostKeyChecking no\\n  UserKnownHostsFile=/dev/null' >> ~/.ssh/config
    fi
    chmod 600 ~/.ssh/config
  " >/dev/null || { log "ERROR: Failed to set SSH config on bastion."; exit 1; }

  scp -o StrictHostKeyChecking=no -i "$KEY_PRIV" src/site.yaml src/alive.py src/service.py "ubuntu@$BASTION_FIP":~ >/dev/null || { log "ERROR: Failed to copy main Ansible files."; exit 1; }
  ssh -o StrictHostKeyChecking=no -i "$KEY_PRIV" "ubuntu@$BASTION_FIP" "mkdir -p ~/templates/" >/dev/null || { log "ERROR: Failed to create templates dir on bastion."; exit 1; }
  scp -o StrictHostKeyChecking=no -i "$KEY_PRIV" src/haproxy.cfg.j2 src/nginx.conf.j2 src/nodes.txt.j2 "ubuntu@$BASTION_FIP":~/templates/ >/dev/null || { log "ERROR: Failed to copy templates."; exit 1; }
  log "Bastion prepared."

  log "Running Ansible playbook on Bastion..."
  ssh -o StrictHostKeyChecking=no -i "$KEY_PRIV" "ubuntu@$BASTION_FIP" /bin/bash <<'EOF'
  set -e 
  log_file='ansible_run.log'
  echo "$(date '+%Y-%m-%d %H:%M:%S') Starting Ansible playbook..." > "$log_file"
  export ANSIBLE_HOST_KEY_CHECKING=False
  if ! command -v ansible-playbook &> /dev/null; then
    echo "$(date '+%Y-%m-%d %H:%M:%S') ERROR: ansible-playbook command not found on bastion." >> "$log_file"
    exit 1
  fi
  ansible-playbook -i ~/hosts.ini ~/site.yaml -v >> "$log_file" 2>&1
  echo "$(date '+%Y-%m-%d %H:%M:%S') Ansible playbook completed successfully." >> "$log_file"
EOF
  if [ $? -ne 0 ]; then
    log "ERROR: Ansible playbook run on Bastion failed. Check 'ansible_run.log' on $BASTION_FIP."
    exit 1
  fi
  log "Ansible playbook run complete."
}

wait_for_os_node_count() {
  local expected_count="$1"
  local timeout=90
  local interval=5
  local waited=0
  log "Waiting for OpenStack to report $expected_count active nodes matching name '$_node'..."
  while true; do
    sleep 2 
    current_os_count=$(openstack server list --long --status ACTIVE --name "$_node" -f value -c Name | wc -l)
    if [[ "$current_os_count" -eq "$expected_count" ]]; then
      log "OpenStack now reports $current_os_count active nodes (matches expected $expected_count)."
      return 0
    fi
    if [ "$waited" -ge "$timeout" ]; then
      log "ERROR: Timeout waiting for OpenStack to report $expected_count active nodes. Current: $current_os_count."
      return 1
    fi
    log "  OpenStack reports $current_os_count nodes. Waiting..."
    sleep "$interval"
    waited=$((waited + interval))
  done
}

_FIPS_LOGGED_ONCE="" 
get_fips_and_internal_ips 
log "Performing initial SSH checks for Bastion and Proxy Floating IPs (essential for operation)..."
if ! wait_for_ssh "$BASTION_FIP"; then
  log "ERROR: Bastion SSH not accessible via FIP. Cannot proceed."
  exit 1
fi
if ! wait_for_ssh "$PROXY_FIP"; then
  log "WARNING: Proxy SSH not directly accessible via FIP ($PROXY_FIP)."
fi

while true; do
  log "-------------------------------------------------------------------"
  log "Starting new operations cycle."

  get_fips_and_internal_ips 

  N_NODES_CONFIG=$(head -n1 servers.conf 2>/dev/null || echo "3")
  DESIRED_NODES=${N_NODES_CONFIG:-3} 
  log "Configuration requires: $DESIRED_NODES nodes."

  log "Pausing for 15 seconds to allow OpenStack API to synchronize before initial check..."
  sleep 15

  declare -A os_nodes_by_name=()
  declare -A bastion_alive_nodes_by_ip=()

  log "DEBUG: Fetching OpenStack server list for active nodes matching name pattern '$_node'..."
  OS_SERVER_LIST_RAW=$(openstack server list --long --status ACTIVE --name "$_node" -f value -c Name -c Status -c Networks || echo "ERROR_FETCHING_SERVERS")
  if [[ "$OS_SERVER_LIST_RAW" == "ERROR_FETCHING_SERVERS" ]]; then
      log "CRITICAL: Failed to fetch server list from OpenStack. Retrying next cycle."
      sleep 30
      continue
  fi
  log "DEBUG: Raw OpenStack server list output:\n$OS_SERVER_LIST_RAW"

  mapfile -t os_server_lines < <(echo "$OS_SERVER_LIST_RAW")
  for line in "${os_server_lines[@]}"; do
    [[ -z "$line" ]] && continue 
    name="${line%% *}"
    status_from_line=$(echo "$line" | awk '{print $2}') 
    ip=$(echo "$line" | grep "$NET" | grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}' | head -1 || true)

    log "DEBUG: Processing line: '$line'"
    log "DEBUG:   Name: '$name', Status: '$status_from_line', IP: '$ip'"
    
    name_cleaned=$(echo "$name" | tr -cd '[:alnum:]_-') 
    log "DEBUG:   Cleaned Name: '$name_cleaned'"

    if [[ -n "$name_cleaned" && -n "$ip" && "$status_from_line" == "ACTIVE" ]]; then
      os_nodes_by_name["$name_cleaned"]="$ip"
      log "DEBUG:   Added '$name_cleaned' (IP: $ip) to os_nodes_by_name. Current keys: ${!os_nodes_by_name[*]}"
    else
      log "WARNING: Found server '$name' (Status: '$status_from_line') but could not extract valid IP or status not ACTIVE. Skipping."
    fi
  done
  CURRENT_OS_NODES_COUNT=${#os_nodes_by_name[@]}
  log "DEBUG: Final os_nodes_by_name content (${CURRENT_OS_NODES_COUNT} nodes): ${!os_nodes_by_name[*]}"

  BASTION_STATUS_OUTPUT=$(ssh -o StrictHostKeyChecking=no -i "$KEY_PRIV" "ubuntu@$BASTION_FIP" "curl -s http://localhost:5000/" 2>/dev/null || echo "ERROR: Bastion service unreachable")
  if [[ "$BASTION_STATUS_OUTPUT" == "ERROR: Bastion service unreachable" ]]; then
    log "WARNING: Could not connect to Bastion's Flask service. Assuming 0 nodes alive."
  else
    mapfile -t alive_ips < <(echo -e "$BASTION_STATUS_OUTPUT" | grep ':ALIVE' | cut -d':' -f1)
    for ip_address in "${alive_ips[@]}"; do
      [[ -z "$ip_address" ]] && continue 
      bastion_alive_nodes_by_ip["$ip_address"]="ALIVE"
    done
    log "Bastion reports ${#alive_ips[@]} nodes available. IPs: ${!bastion_alive_nodes_by_ip[*]}"
  fi

  declare -a operational_node_names=() 
  log "DEBUG: os_nodes_by_name (from OpenStack): ${!os_nodes_by_name[*]}"
  log "DEBUG: bastion_alive_nodes_by_ip (from Bastion): ${!bastion_alive_nodes_by_ip[*]}"

  for name in "${!os_nodes_by_name[@]}"; do
    ip="${os_nodes_by_name[$name]}"
    log "DEBUG: Checking node $name (IP: $ip) for operational status."
    
    current_bastion_status="NOT_FOUND" 
    if [[ -v "bastion_alive_nodes_by_ip[$ip]" ]]; then
        current_bastion_status="${bastion_alive_nodes_by_ip[$ip]}"
    fi

    if [[ "$current_bastion_status" == "ALIVE" ]]; then
      log "DEBUG: Node $name (IP: $ip) is ALIVE. Adding to operational_node_names."
      operational_node_names+=("$name")
    else
      log "DEBUG: Node $name (IP: $ip) is NOT operational (Bastion status: $current_bastion_status)."
    fi
  done
  CURRENT_OPERATIONAL_NODES_COUNT=${#operational_node_names[@]}

  log "OpenStack reports $CURRENT_OS_NODES_COUNT nodes. Desired: $DESIRED_NODES."
  log "Nodes considered operational (OpenStack ACTIVE + Bastion ALIVE): $CURRENT_OPERATIONAL_NODES_COUNT."

  REDEPLOY_NEEDED=false

  if (( CURRENT_OPERATIONAL_NODES_COUNT < DESIRED_NODES )); then
    log "Insufficient operational nodes ($CURRENT_OPERATIONAL_NODES_COUNT) for desired ($DESIRED_NODES)."
    declare -a unhealthy_nodes_to_replace=() 
    for name in "${!os_nodes_by_name[@]}"; do
      ip="${os_nodes_by_name[$name]}"
      bastion_node_status="NOT_FOUND" 
      if [[ -v "bastion_alive_nodes_by_ip[$ip]" ]]; then
          bastion_node_status="${bastion_alive_nodes_by_ip[$ip]}"
      fi
      if [[ "$bastion_node_status" != "ALIVE" ]]; then
        log "INFO: Node $name (IP: $ip) in OpenStack but not ALIVE per Bastion (status: $bastion_node_status). Marking for replacement."
        unhealthy_nodes_to_replace+=("$name")
      fi
    done
    NODES_TO_REPLACE_COUNT=${#unhealthy_nodes_to_replace[@]} 
    if (( NODES_TO_REPLACE_COUNT > 0 )); then
      log "Replacing $NODES_TO_REPLACE_COUNT unhealthy/non-responsive nodes."
      for node_name_to_remove in "${unhealthy_nodes_to_replace[@]}"; do
        log "Preparing to remove unhealthy node: $node_name_to_remove."
        if ! openstack server show "$node_name_to_remove" >/dev/null 2>&1; then
          log "WARNING: Server $node_name_to_remove not found in OpenStack. Skipping."
          REDEPLOY_NEEDED=true 
          continue
        fi
        VOL_TO_DELETE_ID=""
        SERVER_VOLS_JSON=$(openstack server show "$node_name_to_remove" -f json -c volumes_attached 2>/dev/null || echo "{}") 
        if command -v jq &>/dev/null; then
          VOL_TO_DELETE_ID=$(echo "$SERVER_VOLS_JSON" | jq -r '.volumes_attached[0].id // empty' 2>/dev/null || echo "")
        else
          VOL_TO_DELETE_ID=$(echo "$SERVER_VOLS_JSON" | grep -oP '"volumes_attached":\s*\[\s*\{\s*"id":\s*"\K[a-f0-9-]+' | head -1 || echo "")
        fi
        log "Attempting to delete server $node_name_to_remove..."
        openstack server delete "$node_name_to_remove" --wait >/dev/null
        if [ $? -ne 0 ]; then
          log "WARNING: Failed to delete server $node_name_to_remove. Skipping volume deletion."
          continue 
        fi
        log "Server $node_name_to_remove deleted."
        if [[ -n "$VOL_TO_DELETE_ID" && "$VOL_TO_DELETE_ID" != "null" ]]; then 
          log "Found attached volume $VOL_TO_DELETE_ID for $node_name_to_remove. Attempting to delete..."
          openstack volume delete "$VOL_TO_DELETE_ID" >/dev/null 2>&1 || log "WARNING: Initial volume delete for $VOL_TO_DELETE_ID failed."
            MAX_WAIT_TIME=120; WAIT_INTERVAL=5; ELAPSED_TIME=0
            while openstack volume show "$VOL_TO_DELETE_ID" &>/dev/null && (( ELAPSED_TIME < MAX_WAIT_TIME )); do
                vol_status="" 
                vol_status=$(openstack volume show "$VOL_TO_DELETE_ID" -f value -c status 2>/dev/null || echo "not_found")
                if [[ "$vol_status" == "deleting" ]]; then
                    log "  Volume $VOL_TO_DELETE_ID status: $vol_status. Waiting..."
                elif [[ "$vol_status" == "available" && $(openstack volume show "$VOL_TO_DELETE_ID" -f value -c "attach_information" 2>/dev/null) == "" ]]; then 
                    log "  Volume $VOL_TO_DELETE_ID status: $vol_status and detached. Retrying delete."
                    openstack volume delete "$VOL_TO_DELETE_ID" >/dev/null 2>&1 || true
                elif [[ "$vol_status" == "error" || "$vol_status" == "error_deleting" ]]; then
                    log "  Volume $VOL_TO_DELETE_ID status: $vol_status. Attempting delete again."
                    openstack volume delete "$VOL_TO_DELETE_ID" >/dev/null 2>&1 || true 
                    break 
                elif [[ "$vol_status" == "not_found" ]]; then
                    log "  Volume $VOL_TO_DELETE_ID no longer found."
                    break 
                else 
                    log "  Volume $VOL_TO_DELETE_ID status: $vol_status. Waiting."
                fi
                sleep "$WAIT_INTERVAL"
                ELAPSED_TIME=$((ELAPSED_TIME + WAIT_INTERVAL))
            done
            if openstack volume show "$VOL_TO_DELETE_ID" &>/dev/null; then
                log "WARNING: Volume $VOL_TO_DELETE_ID for $node_name_to_remove did not delete. Status: $(openstack volume show $VOL_TO_DELETE_ID -f value -c status 2>/dev/null || echo "not_found")."
            else
                log "Volume $VOL_TO_DELETE_ID for $node_name_to_remove deleted or confirmed gone."
            fi
        else
          log "No uniquely identified attached volume ID for $node_name_to_remove."
        fi
        log "$node_name_to_remove and associated resources (attempted) removed."
        REDEPLOY_NEEDED=true
        log "Pausing for 10 seconds after processing $node_name_to_remove..."
        sleep 10
        CURRENT_OS_NODES_COUNT=$(openstack server list --long --status ACTIVE --name "$_node" -f value -c Name | wc -l)
      done
    fi 
    log "DEBUG: Re-querying OpenStack for current node count before adding new ones."
    mapfile -t current_os_node_names_list < <(openstack server list --long --status ACTIVE --name "$_node" -f value -c Name)
    CURRENT_OS_NODES_COUNT=${#current_os_node_names_list[@]}
    log "DEBUG: Refreshed CURRENT_OS_NODES_COUNT: $CURRENT_OS_NODES_COUNT"

    if (( CURRENT_OS_NODES_COUNT < DESIRED_NODES )); then
      NODES_TO_ADD=$((DESIRED_NODES - CURRENT_OS_NODES_COUNT))
      log "Adding $NODES_TO_ADD new nodes to reach desired count of $DESIRED_NODES."
      highest_node_num=0
      mapfile -t existing_node_names < <(openstack server list --status ACTIVE --name "$_node" -f value -c Name)
      for name in "${existing_node_names[@]}"; do
        node_num_str=$(echo "$name" | sed -E "s/^${_node}([0-9]+)$/\1/" || echo "")
        if [[ "$node_num_str" =~ ^[0-9]+$ ]]; then 
          if (( node_num_str > highest_node_num )); then
            highest_node_num=$node_num_str
          fi
        fi
      done
      IMG_ID=$(openstack image list --status active -f value -c ID -c Name \
        | grep -i 'ubuntu' | grep '20.04' | sort -k2 | tail -n1 | awk '{print $1}' || true)
      if [[ -z "$IMG_ID" ]]; then log "ERROR: Could not find Ubuntu 20.04 image. Exiting."; exit 1; fi
      FLAVOR="b.1c2gb" 
      BOOT_VOL_SIZE=10 
      if ! openstack security group show "$_sg_nodes" >/dev/null 2>&1; then
        log "ERROR: Security group $_sg_nodes not found. Exiting."
        exit 1
      fi
      for i in $(seq 1 "$NODES_TO_ADD"); do
        new_node_num=$((highest_node_num + i))
        NEW_NODE_NAME="${_node}${new_node_num}"
        VOL_NAME="${TAG}_vol_${NEW_NODE_NAME}" 
        log "Launching new node: $NEW_NODE_NAME (Volume: $VOL_NAME)"
        NEW_VOL_ID=$(openstack volume create \
          --size "$BOOT_VOL_SIZE" \
          --image "$IMG_ID" \
          --property tag="$TAG" \
          --description "$TAG disk for $NEW_NODE_NAME" \
          "$VOL_NAME" -f value -c id 2>/dev/null || echo "") 
        if [[ -z "$NEW_VOL_ID" ]]; then
          log "ERROR: Failed to create volume $VOL_NAME for $NEW_NODE_NAME. Skipping."
          continue
        fi
        log "Volume $VOL_NAME ($NEW_VOL_ID) created. Waiting for availability..."
        vol_wait_timeout=120; vol_wait_interval=5; vol_waited=0; vol_available=false
        while (( vol_waited < vol_wait_timeout )); do
            vol_status=$(openstack volume show "$NEW_VOL_ID" -f value -c status 2>/dev/null || echo "error")
            if [[ "$vol_status" == "available" ]]; then
                vol_available=true; break
            elif [[ "$vol_status" == "error" ]]; then
                log "ERROR: Volume $NEW_VOL_ID for $NEW_NODE_NAME in error state."
                break 
            fi
            log "  Volume $NEW_VOL_ID status: $vol_status. Waiting..."
            sleep $vol_wait_interval
            vol_waited=$((vol_waited + vol_wait_interval))
        done
        if ! $vol_available; then
            log "ERROR: Volume $NEW_VOL_ID for $NEW_NODE_NAME not available. Deleting volume."
            openstack volume delete "$NEW_VOL_ID" >/dev/null 2>&1 || true 
            continue
        fi
        OPENSTACK_KEYPAIR_NAME="${TAG}_key"
        log "Volume $NEW_VOL_ID is available."
        openstack server create \
          --flavor "$FLAVOR" \
          --key-name "$OPENSTACK_KEYPAIR_NAME" \
          --network "$NET" \
          --security-group "$_sg_nodes" \
          --property tag="$TAG" \
          --volume "$NEW_VOL_ID" \
          --wait \
          "$NEW_NODE_NAME" >/dev/null
        if [ $? -ne 0 ]; then
          log "ERROR: Failed to create server $NEW_NODE_NAME. Deleting volume $NEW_VOL_ID."
          openstack volume delete "$NEW_VOL_ID" >/dev/null 2>&1 || true 
          continue
        fi
        log "$NEW_NODE_NAME launched and active."
        NEW_NODE_INTERNAL_IP=$(openstack server show "$NEW_NODE_NAME" -f value -c addresses | grep "$NET" | grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}' | head -1 || true)
        if [[ -z "$NEW_NODE_INTERNAL_IP" ]]; then
          log "ERROR: Could not get internal IP for $NEW_NODE_NAME. Deleting server and volume."
          openstack server delete "$NEW_NODE_NAME" --wait >/dev/null 2>&1 || true
          openstack volume delete "$NEW_VOL_ID" >/dev/null 2>&1 || true
          continue
        fi
        log "Waiting for SSH from Bastion to new node $NEW_NODE_NAME ($NEW_NODE_INTERNAL_IP)..."
        if ! wait_for_internal_ssh_from_bastion "$NEW_NODE_INTERNAL_IP"; then
          log "ERROR: SSH to $NEW_NODE_NAME ($NEW_NODE_INTERNAL_IP) from Bastion failed. Deleting node and exiting."
          openstack server delete "$NEW_NODE_NAME" --wait >/dev/null 2>&1 || true
          openstack volume delete "$NEW_VOL_ID" >/dev/null 2>&1 || true
          exit 1 
        fi
        log "SSH access to $NEW_NODE_NAME from Bastion is ready."
        REDEPLOY_NEEDED=true
      done 
      log "Node addition process complete."
      if ! wait_for_os_node_count "$DESIRED_NODES"; then
        log "ERROR: OpenStack node count not $DESIRED_NODES after additions. Current $(openstack server list --long --status ACTIVE --name "$_node" -f value -c Name | wc -l). Exiting."
        exit 1
      fi
    fi 
  elif (( CURRENT_OS_NODES_COUNT > DESIRED_NODES )); then
    NODES_TO_REMOVE_COUNT=$((CURRENT_OS_NODES_COUNT - DESIRED_NODES)) 
    log "Scaling down: Need to remove $NODES_TO_REMOVE_COUNT surplus nodes."
    declare -a all_os_node_names_list=() 
    for name in "${!os_nodes_by_name[@]}"; do
      all_os_node_names_list+=("$name")
    done
    IFS=$'\n' sorted_os_nodes_to_consider_for_removal=($(
      for node_name_iter in "${all_os_node_names_list[@]}"; do
        node_num_str=$(echo "$node_name_iter" | sed -E "s/^${_node}([0-9]+)$/\1/" || echo "0") 
        [[ ! "$node_num_str" =~ ^[0-9]+$ ]] && node_num_str=0 
        echo "$node_num_str $node_name_iter"
      done | sort -rnk1,1 | awk '{print $2}'
    ))
    unset IFS
    nodes_to_terminate=("${sorted_os_nodes_to_consider_for_removal[@]:0:$NODES_TO_REMOVE_COUNT}") 
    for node_name_to_remove in "${nodes_to_terminate[@]}"; do
      log "Preparing to remove surplus node: $node_name_to_remove."
        if ! openstack server show "$node_name_to_remove" >/dev/null 2>&1; then
          log "WARNING: Server $node_name_to_remove not found. Skipping."
          REDEPLOY_NEEDED=true 
          continue
        fi
        VOL_TO_DELETE_ID=""
        SERVER_VOLS_JSON=$(openstack server show "$node_name_to_remove" -f json -c volumes_attached 2>/dev/null || echo "{}")
        if command -v jq &>/dev/null; then
          VOL_TO_DELETE_ID=$(echo "$SERVER_VOLS_JSON" | jq -r '.volumes_attached[0].id // empty' 2>/dev/null || echo "")
        else
          VOL_TO_DELETE_ID=$(echo "$SERVER_VOLS_JSON" | grep -oP '"volumes_attached":\s*\[\s*\{\s*"id":\s*"\K[a-f0-9-]+' | head -1 || echo "")
        fi
        log "Attempting to delete server $node_name_to_remove..."
        openstack server delete "$node_name_to_remove" --wait >/dev/null
        if [ $? -ne 0 ]; then
          log "WARNING: Failed to delete server $node_name_to_remove. Skipping volume."
          continue
        fi
        log "Server $node_name_to_remove deleted."
        if [[ -n "$VOL_TO_DELETE_ID" && "$VOL_TO_DELETE_ID" != "null" ]]; then
          log "Found attached volume $VOL_TO_DELETE_ID for $node_name_to_remove. Deleting..."
           openstack volume delete "$VOL_TO_DELETE_ID" >/dev/null 2>&1 || log "WARNING: Initial volume delete for $VOL_TO_DELETE_ID failed."
            MAX_WAIT_TIME=120; WAIT_INTERVAL=5; ELAPSED_TIME=0
            while openstack volume show "$VOL_TO_DELETE_ID" &>/dev/null && (( ELAPSED_TIME < MAX_WAIT_TIME )); do
                vol_status=""; vol_status=$(openstack volume show "$VOL_TO_DELETE_ID" -f value -c status 2>/dev/null || echo "not_found") 
                if [[ "$vol_status" == "deleting" || "$vol_status" == "not_found" ]]; then break; fi
                log "  Volume $VOL_TO_DELETE_ID status: $vol_status. Waiting..."; sleep "$WAIT_INTERVAL"; ELAPSED_TIME=$((ELAPSED_TIME + WAIT_INTERVAL))
            done
            if openstack volume show "$VOL_TO_DELETE_ID" &>/dev/null; then log "WARNING: Volume $VOL_TO_DELETE_ID for $node_name_to_remove did not delete."; else log "Volume $VOL_TO_DELETE_ID deleted."; fi
        else
          log "No uniquely identified attached volume ID for $node_name_to_remove."
        fi
      log "$node_name_to_remove and associated resources removed."
      REDEPLOY_NEEDED=true
      log "Pausing for 10 seconds after processing $node_name_to_remove."
      sleep 10
    done 
    log "Surplus node removal process complete."
    if ! wait_for_os_node_count "$DESIRED_NODES"; then
      log "ERROR: OpenStack node count not $DESIRED_NODES after scaling down. Exiting."
      exit 1
    fi
  fi 

  if $REDEPLOY_NEEDED; then
    log "Scaling actions occurred. Updating playbook and configuration via Ansible."
    update_and_run_ansible
    log "Ansible run complete."
    log "Pausing for 10 seconds for API service sync after Ansible run..."
    sleep 10
  else
    log "No scaling actions were needed in this cycle."
  fi

  log "Cycle finished. Sleeping for 30 seconds before next operations cycle."
  sleep 30
done